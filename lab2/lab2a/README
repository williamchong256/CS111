NAME: William Chong
EMAIL: williamchong256@gmail.com
ID: 205114665

Contents of tarball:

lab2_add.c: C source code to implement and test a shared global variable. Uses locks and multithreading.

lab2_list.c: C source code that uses multithreading to drive the SortedList.c doubly-linked, circular linked
	list. Has command line options to specify num of threads, iterations, yields, and sync protection.

SortedList.h: header file that describes the interface for the linked list operations

SortedList.c: C module that implements the linked list operations specified in the header file.

data_generator.sh: a test script that runs various combinations of commandline options on lab2_add and
	lab2_list and outputs them to .csv files.

lab2_add.csv:
lab2_list.csv:
	These are the CSV test outputs for lab2_add and lab2_list.

lab2_add.gp:
lab2_list.gp:
	These scripts generate the graphs using gnuplot from the data in the .csv files.

.png files.
	- add-1: Threads and iterations that run without failure
	- add-2: Cost of yielding 
	- add-3: Single threaded operation cost vs number of iterations
	- add-4: Threads and iterations (with sync options) that run without failure
	- add-5: Per operation cost vs number of threads for protected opeerations.

	- list-1: Average cost per operation for single threaded operation (unprotected)
	- list-2: Unprotected threads and iterations that run without failure (different yield options)
	- list-3: Protected Iterations that run without failure.
	- list-4: Scalability of mutexes vs spin-locks. (cost per operation).

Makefile:
	default: compiles all programs with -Wall and -Wextra options
	build:   same as default

	tests: executes data_generator.sh script to test lab2_add and lab2_list; outputs to .csv files
	graphs: uses supplied .gp scripts to generate graphs
	dist: creates tarball with the above files
	clean: restores directory to freshly untarred state.

Sources:
  - clock_gettime(3) manpage
  - POSIX Threads Programming: https://computing.llnl.gov/tutorials/pthreads/#Thread
  - Operating Systems (class textbook) Chapters 27 and 28: "Thread API" and "Locks"
  - gnuplot manpage

------------------QUESTIONS--------------------

Question 2.1.1:
  - Why does it take many iterations before errors are seen?
	With a low number of iterations each thread is more likely to finish
	its workload and exit the critical section (incrementing the counter
	global variable) before being preempted by another thread. However
	with a higher number of iterations (greater than 1000), then each thread
	fairly consistently is in the middle of iterating the counter when it
	is preempted by another thread which causes the race condition error.
	The interrupting thread then accesses the global variable and modifies
	it, leading to erroneous counter values at the end.
  - Why does a significantly smaller number of iterations so seldom fail?
	With a significantly smaller number of iterations, each thread is 
	likely to be able to finish its iterations and exit the critical
	section before a race condition can develop.

Question 2.1.2
  - Why are the --yield runs so much slower?
	The --yield runs are slower than add-none runs because the --yield option causes
	threads to more readily, and frequently, yield their time slice to other threads.
	Since the cost of thread context switching is expensive, more frequent yields 
	by each thread means more overhead.
  - Where is the additional time going?
	The additional time is spent doing thread context switches--saving and restoring
	register values. 
  - Is it possible to get valid per-operation timings if we are using the --yield option?
    If so, explain how. If not, explain why not.
	It isn't possible to get accurate per-operation timings with the --yield option 
	because the timing being measured is including not only the actual operation,
	but is also recording the time it takes to perform the context switches, which 
	adds significant overhead that is not the actual runtime of the operations. It is
	difficult (perhaps very difficult) to get an accurate measure of the time cost
	for context switch overhead, and thus cannot get an accurate measure of the
	per-operation timing.

Question 2.1.3
  - Why does the average cost per operation drop with increasing iterations?
	The average cost per operation drops with increasing iterations because for 
	smaller numbers of iterations, the overall cost is dominated more by the 
	cost of creating and joining threads. With higher numbers of iterations,
	the cost of creating and joining threads remains constant, while the time
	to perform the workload/operations dominates the timing. Thus, the average
	cost per operation is decreased.
  - If the cost per iteration is a function of the number of iterations, how do we know how 
    many iterations to run (or what the "correct" cost is)?
	To determine the accurate cost per iteration (operation), we would run a very large
	number of iterations so that the overhead of managing threads is far outweighed
	by the actual time cost of performing the operations. As number of iterations 
	increases, the cost per operation approaches the "correct cost".

Question 2.1.4
  - Why do all of the options perform similarly for low numbers of threads?
	With low numbers of threads, there are less threads contending for the protected
	critical section. With less threads, there is less testing/checking if the lock
	is free to be acquired. Thus there is less time overhead due to waiting for a 
	resource.
  - Why do the three protected operations slow down as the number of threads rises?
	As the number of threads increases, each thread spends more time checking or
	waiting for the resource to be freed up. The increased number of thread means
	more threads attempt to acquire a lock, which leads to slow down.

Question 2.2.1
  - Compare the variation in time per mutex-protected operation vs the number of threads
    in Part 1 and Part 2
  - Comment on the general shapes of the curves, and explain why they have this shape.
  - Comment on the relative rates of increase and differences in the shapes of the curves,
    and offer an explanation for these differences.
	Part 2's mutex protected operation, similar to Part 1's, similarly increases in
	time cost per mutex protected operation as number of threads increases, but
	seems to vary less drastically. The add operation cost curve initially steeply
	increases in cost, but begins to plateau with increasing threads; the list operation
	cost curve remains linear throughout. The reason for this could be that add operations
	are not very CPU intensive, so the cost of obtaining a lock dominates the cost/op, while
	with list operations, the cost of inserts, lookups, and deletes are much more CPU intensive
	leading to the mutex protection cost not making as large of an impact on overall cost/op, 
	thus only increasingly linearly due to the constant cost of having the lock.

Question 2.2.2
  Compare the variation in time per protected operation vs the number of threads for 
  list operations protected by Mutex vs Spin locks. 
  Comment on the general shapes of the curves, and explain why they have this shape.
  Comment on the relative rates of increase and differences in the shapes of the curves, and 
  offer an explanation for these differences.

	As seen in add-5.png the time cost per protected operation for mutexes levels off
	after an initial increase, while the cost per protected operation with spin locks
	increases more and more with increasing number of threads. In the list, the spin
	locks would also eventually take over mutexes in time cost overhead, but does so
	more slowly since the cost for the actual list operations dominates rather than 
	thread overheads. The add-5.png spin lock cost curve has a steep upward slope
	which indicates that with increasing numbers of threads, each thread spends
	more time spin waiting, waiting for the lock to be freed up--leading to significant
	time waste.
	
